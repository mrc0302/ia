import streamlit as st
import google.generativeai as genai
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
import numpy as np
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
import faiss
import os

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Chat com FAISS e Gemini",
    page_icon="ü§ñ",
    layout="wide"
)

# T√≠tulo da aplica√ß√£o
st.title("ü§ñ Chat com FAISS e Gemini")
st.markdown("---")

# Sidebar para configura√ß√µes
with st.sidebar:
    st.header("‚öôÔ∏è Configura√ß√µes")
    
    # Campo para API Key
    api_key = st.text_input(
        "Google API Key:", 
        type="password",
        value="AIzaSyBJ9ifJikSVNqF7njMoc-wlcIrdjLWcvY4",
        help="Insira sua chave da API do Google"
    )
    
    # Campo para caminho do FAISS
    caminho_faiss = st.text_input(
        "Caminho do FAISS:",
        value="faiss_legal_store_gemini_faiss_legal_store_gemini",
        help="Caminho para o diret√≥rio do √≠ndice FAISS"
    )
    
    # N√∫mero de documentos para busca
    k_docs = st.slider(
        "N√∫mero de documentos para busca:",
        min_value=1,
        max_value=10,
        value=3,
        help="Quantos documentos similares buscar"
    )
    
    # Bot√£o para carregar FAISS
    if st.button("üîÑ Carregar FAISS", type="primary"):
        if api_key and caminho_faiss:
            with st.spinner("Carregando FAISS..."):
                try:
                    # Configurar API key
                    genai.configure(api_key=api_key)
                    
                    # Configurar embeddings
                    embeddings = GoogleGenerativeAIEmbeddings(
                        model="models/embedding-001",
                        google_api_key=api_key
                    )
                    
                    # Carregar FAISS
                    vector_store = FAISS.load_local(
                        caminho_faiss,
                        embeddings,
                        allow_dangerous_deserialization=True
                    )
                    
                    # Salvar no session state
                    st.session_state.vector_store = vector_store
                    st.session_state.embeddings = embeddings
                    st.session_state.api_key = api_key
                    
                    st.success("‚úÖ FAISS carregado com sucesso!")
                    st.info(f"üìä N√∫mero de vetores: {vector_store.index.ntotal}")
                    st.info(f"üìê Dimens√£o: {vector_store.index.d}")
                    
                except Exception as e:
                    st.error(f"‚ùå Erro ao carregar FAISS: {str(e)}")
        else:
            st.warning("‚ö†Ô∏è Preencha a API Key e o caminho do FAISS")

# √Årea principal do chat
if 'vector_store' in st.session_state:
    st.header("üí¨ Chat")
    
    # Inicializar hist√≥rico de mensagens
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    # Exibir hist√≥rico de mensagens
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Input do usu√°rio
    if prompt := st.chat_input("Digite sua pergunta..."):
        # Adicionar mensagem do usu√°rio ao hist√≥rico
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Processar pergunta
        with st.chat_message("assistant"):
            with st.spinner("Processando..."):
                try:
                    # Configurar modelo Gemini
                    llm = genai.GenerativeModel(
                        model_name="gemini-2.0-flash-exp",
                        generation_config={
                            "temperature": 0.1,
                            "top_p": 0.9,
                            "top_k": 4,
                            "max_output_tokens": 8192,
                            "response_mime_type": "text/plain",
                        }
                    )
                    
                    # Buscar documentos similares
                    docs = st.session_state.vector_store.similarity_search(prompt, k=k_docs)
                    contexto = "\n".join([doc.page_content for doc in docs])
                    
                    # Criar prompt
                    full_prompt = f"Contexto: {contexto}\n\nPergunta: {prompt}\n\nResposta:"
                    
                    # Gerar resposta
                    response = llm.generate_content(full_prompt)
                    resposta = response.text
                    
                    # Exibir resposta
                    st.markdown(resposta)
                    
                    # Adicionar resposta ao hist√≥rico
                    st.session_state.messages.append({"role": "assistant", "content": resposta})
                    
                except Exception as e:
                    st.error(f"‚ùå Erro ao processar pergunta: {str(e)}")
    
    # Bot√£o para limpar hist√≥rico
    if st.button("üóëÔ∏è Limpar Hist√≥rico"):
        st.session_state.messages = []
        st.rerun()
    
    # Se√ß√£o de informa√ß√µes dos documentos encontrados
    if st.session_state.messages:
        with st.expander("üìÑ Documentos Utilizados na √öltima Consulta"):
            if 'vector_store' in st.session_state and len(st.session_state.messages) > 0:
                try:
                    ultima_pergunta = None
                    for msg in reversed(st.session_state.messages):
                        if msg["role"] == "user":
                            ultima_pergunta = msg["content"]
                            break
                    
                    if ultima_pergunta:
                        docs = st.session_state.vector_store.similarity_search(ultima_pergunta, k=k_docs)
                        for i, doc in enumerate(docs, 1):
                            st.markdown(f"**Documento {i}:**")
                            st.text_area(
                                f"Conte√∫do {i}",
                                doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content,
                                height=100,
                                key=f"doc_{i}"
                            )
                            if hasattr(doc, 'metadata') and doc.metadata:
                                st.json(doc.metadata)
                            st.markdown("---")
                except Exception as e:
                    st.error(f"Erro ao exibir documentos: {str(e)}")

else:
    st.info("üëà Configure a API Key e carregue o √≠ndice FAISS na barra lateral para come√ßar o chat.")
    
    # Se√ß√£o de ajuda
    with st.expander("‚ÑπÔ∏è Como usar"):
        st.markdown("""
        ### Passos para usar a aplica√ß√£o:
        
        1. **Configure a API Key**: Insira sua chave da API do Google Gemini na barra lateral
        2. **Defina o caminho do FAISS**: Especifique o diret√≥rio onde est√° salvo seu √≠ndice FAISS
        3. **Ajuste o n√∫mero de documentos**: Escolha quantos documentos similares buscar (padr√£o: 3)
        4. **Carregue o FAISS**: Clique no bot√£o "Carregar FAISS"
        5. **Inicie o chat**: Digite suas perguntas na √°rea de chat
        
        ### Recursos dispon√≠veis:
        - üí¨ **Chat interativo** com hist√≥rico de mensagens
        - üìÑ **Visualiza√ß√£o dos documentos** utilizados em cada consulta
        - ‚öôÔ∏è **Configura√ß√µes ajust√°veis** na barra lateral
        - üóëÔ∏è **Limpeza do hist√≥rico** quando necess√°rio
        """)

# Footer
st.markdown("---")
st.markdown("Desenvolvido com ‚ù§Ô∏è usando Streamlit, LangChain e Google Gemini")